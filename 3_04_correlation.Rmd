# A correlation test

We're going to consider 'correlation' in this chapter. A correlation is a statistical measure of **association** between two variables. An association is any relationship between the variables that makes them dependent in some way: knowing the value of one variable gives you information about the possible values of the other.

The terms 'association' and 'correlation' are often used interchangeably but strictly speaking, correlation has a narrower definition. Correlation analysis quantifies the degree to which an association tends to a certain pattern via a measure called a **correlation coefficient**. For example, the correlation coefficient studied below---Pearson's correlation coefficient---measures the degree to which two variables tend toward a straight line relationship.

```{r, echo=FALSE}
set.seed(25081978)
x <- data.frame(x = rnorm(50))
cor_eg <- bind_rows(
  mutate(x, y = +x * 0.6, 
         labs = "cor = +1.0"),
  mutate(x, y = +x * 0.35 + rnorm(n(), sd = 0.48), 
         labs = "cor = +0.6"),
  mutate(x, y = rnorm(n(), sd = .6), 
         labs = "cor = +0.0"),
  mutate(x, y = -x * 0.35 + rnorm(n(), sd = 0.48), 
         labs = "cor = -0.6"),
  mutate(x, y = -x * 0.6, 
         labs = "cor = -1.0")
) 
cor_eg <- mutate(cor_eg, 
                 labs = factor(labs, levels = rev(unique(cor_eg$labs))))
```

There are different methods for quantifying correlation, but these all share a number of properties:

1.  If there is no relationship between the variables, the correlation coefficient will be zero. The closer to 0 the value, the weaker the relationship. A perfect correlation will be either -1 or +1, depending on the direction. This is illustrated in the figure below:

```{r cor-strength-eg, echo=FALSE, out.width='80%', fig.asp=0.3, fig.align='center', warning=FALSE}
ggplot(cor_eg, aes(x = x, y = y)) + 
  geom_point() + facet_wrap(~labs, ncol = 5) + 
  scale_x_continuous(limits = c(-1,1) * 3.1) + 
  scale_y_continuous(limits = c(-1,1) * 1.8)
```

2.  The value of a correlation coefficient indicates the direction and strength of the association, but it says nothing about the steepness of the relationship. A correlation coefficient is just a number, so it can't tell us exactly how one variable depends on the other.

3.  Correlation coefficients do not describe 'directional' or 'casual' relationships. We can't use correlations to make predictions about one variable based on knowledge of another or make statements about the effect of one variable on the other.

4.  A correlation coefficient doesn't tell us whether an association is likely to be 'real' or not. We have to use a statistical significance test to evaluate whether a correlation may be different from zero. Like any statistical test, this requires certain assumptions about the variables to be met.

We're going to make sense of all this by studying one particular correlation coefficient in this chapter: **Pearson's product-moment correlation coefficient** ($r$). Various measures of association exist, so why focus on this one? Well... once you know how to work with one type of correlation in R, it isn't hard to use another. Pearson's product-moment correlation coefficient is the most well-known, which means it is as good a place as any to learn about correlation analysis.

## Pearson's product-moment correlation

What do we need to know about Pearson's product-moment correlation? Let's start with the naming conventions. People often use "Pearson's correlation coefficient" or "Pearson's correlation" as a convenient shorthand because writing "Pearson's product-moment correlation coefficient" all the time soon becomes tedious. If we want to be concise, we can use the standard mathematical symbol to denote Pearson's correlation coefficient---lower case '$r$'.

The one thing we absolutely have to know about Pearson's correlation coefficient is that it is **a measure of linear association between numeric variables**. This means Pearson's correlation is appropriate when numeric variables follow a 'straight-line' relationship. That doesn't mean they have to be perfectly related, by the way. It simply means there shouldn't be any 'curviness' to their pattern of association[^correlation-1].

[^correlation-1]: If non-linear associations *are* apparent it's generally better to use a different correlation coefficient (we'll consider one alternative later in the book).

Finally, calculating Pearson's correlation coefficient serves to *estimate* the strength of an association. An estimate can't tell us whether that association is likely to be 'real' or not. We need a statistical test to tackle that question. There is a standard parametric test associated with Pearson's correlation coefficient. Unfortunately, this does not have its own name. We will call it "Pearson's correlation test" to distinguish the test from the coefficient. Just keep in mind these are not 'official' names.

### Pearson's correlation test

The logic underpinning Pearson's correlation test is the same as we've seen in previous tests:

1.  Define a null hypothesis.
2.  Calculate an appropriate test statistic.
3.  Work out the null distribution of that statistic.
4.  Use this to calculate aÂ *p*-value from the observed coefficient.

We won't work through the details other than to note a few important aspects:

-   When working with Pearson's correlation coefficient, the 'no effect' null hypothesis corresponds to one of zero (linear) association between the two variables ($r=0$).
-   The test statistic associated with $r$ turns out to be a *t*-statistic. This has nothing to do with comparing means---*t*-statistics pop up all the time in frequentist statistics.

Like any parametric technique, Pearson's correlation test makes several assumptions. These need to be met in order for the statistical test to be reliable. The assumptions are:

-   Both variables are measured on an interval or ratio scale.
-   The two variables are normally distributed (in the population).
-   The relationship between the variables is linear.

The first two requirements should not need any further explanation at this point---we've seen them before in the context of the one- and two-sample *t*-tests. The third one stems from Pearson's correlation coefficient being a measure of linear association.

Only the linearity assumption needs to be met for Pearson's correlation coefficient ($r$) to be a valid measure of association. As long as the relationship between two variables is linear, $r$ produces a sensible measure of association. However, the first two assumptions need to be met for the associated statistical test to be appropriate.

That's enough background and abstract concepts. Let's see how to perform correlation analysis in R using Pearson's correlation coefficient.

## Pearson's product-moment correlation coefficient in R

::: {.infobox .action data-latex="{action}"}
####  {.unnumbered}

We will use a new data set to demonstrate correlation analysis. The data live in a file called 'BRACKEN.CSV'. The code below assumes those data have been read into a tibble called `bracken`. Set that up if you plan to work along.
:::

```{r, include=FALSE}
bracken <- readr::read_csv("./data_csv/BRACKEN.CSV")
```

The plant morph example is not suitable for correlation analysis. We need a new example to motivate a workflow for correlation tests in R. The example we're going use is about the association between ferns and heather...

Bracken fern (*Pteridium aquilinum*) is a common plant in many upland areas. A land manager needs to know whether there is any association between bracken and heather (*Calluna vulgaris*) in these areas. To determine whether the two species are associated, she sampled 22 plots at random and estimated the density of bracken and heather in each plot. The data are the mean *Calluna* standing crop (g m^-2^) and the number of bracken fronds per m^2^.

### Visualising the data and checking the assumptions

The data are in the file BRACKEN.CSV. Read these data into a data frame, calling it `bracken`:

```{r, eval=FALSE}
bracken <- read_csv("BRACKEN.CSV")
```

```{r}
glimpse(bracken)
```

There are 22 observations (rows) and two variables (columns) in this data set. The two variables, `Calluna` and `Bracken`, contain the estimates of heather and bracken abundance in each plot, respectively.

We should always explore the data thoroughly before carrying out any kind of statistical analysis. To begin, we can visualise the form of the association with a scatter plot:

```{r bracken-calluna-scatter, fig.width=3.5, fig.asp=1, fig.align='center'}
ggplot(bracken, aes(x = Calluna, y = Bracken)) +
  geom_point()
```

There appears to be a strong negative association between the species' abundances, and the relationship seems to follow a 'straight line' pattern. It looks like Pearson's correlation is a reasonable measure of association for these data.

We will confirm this with a significance test. Is it appropriate to carry out the test? We're dealing with numeric variables measured in ratio scale (assumption 1). What about their distributions (assumptions 2)? Here's a quick visual summary:

```{r bracken-calluna-dist, fig.show = 'hold', fig.width=4, fig.asp=1, out.width='40%'}
ggplot(bracken, aes(x = Calluna)) + geom_dotplot(binwidth = 100)
ggplot(bracken, aes(x = Bracken)) + geom_dotplot(binwidth = 2)
```

These dot plots suggest the normality assumption is met, i.e. both distributions are roughly 'bell-shaped'.

That's all three assumptions met---the variables are on a ratio scale, dot the normality assumption is met, and the abundance relationship is linear. It looks like the statistical test will give reliable results.

### Doing the test

Let's proceed with the analysis... Carrying out a correlation analysis in R is straightforward. We use the `cor.test` function to do this:

```{r, eval=FALSE}
cor.test(~ Calluna + Bracken, method = "pearson", data = bracken)
```

We have suppressed the output here to focus on how the function works:

-   We use the R formula syntax to determine which pair of variables are analysed. The `cor.test` function expects the two variable names to appear to the right of the `~`, separated by a `+` symbol, with nothing on the left.

-   We use `method = "pearson"` to control which type of correlation coefficient was calculated. The default method is Pearson's correlation, but it never hurts to be explicit, so we wrote `method = "pearson"` anyway.

-   When we use the formula syntax, as we are doing here, we have to tell the function where to find the variables. That's the `data = bracken` part.

Notice `cor.test` uses a different convention from the `t.test` function for the formula argument of. `t.test` places one variable on the left and the other variable on the right-hand side of the `~` (e.g. `Calluna ~ Bracken`), whereas `cor.test` requires both two variables to appear to the right of the `~`, separated by a `+` symbol. This convention exists to emphasise the absence of 'directionality' in correlations, i.e. neither variable has a special status. This will probably make more sense after covering 'regression' in later chapters.

The output from the `cor.test` is, however, very similar to that produced by the `t.test` function. Here it is:

```{r, echo=FALSE}
cor.test(~ Calluna + Bracken, method = "pearson", data = bracken)
```

We won't step through most of this as its meaning should be clear. The `t = -5.2706, df = 20, p-value = 3.701e-05` line is the one we really care about:

-   The first part says that the test statistic associated with the test is a *t*-statistic, where `t = -5.27`. Remember, this has nothing to do with 'comparing means'.
-   Next, we see the degrees of freedom for the test. Can you see where this comes from? It is $n-2$, where $n$ is the sample size. Together, the degrees of freedom and the *t*-statistic determine the *p*-value.
-   The *t*-statistic and associated *p*-value are generated under the null hypothesis of zero correlation ($r = 0$). Since *p* \< 0.05, we conclude that there is a statistically significant correlation between bracken and heather.

What is the actual correlation between bracken and heather densities? That's given at the bottom of the test output: $-0.76$. As expected from the scatter plot, there is quite a strong negative association between bracken and heather densities.

### Reporting the result

When using Pearson's method, we should report the value of the correlation coefficient, the sample size, and the *p*-value[^correlation-2]. Here's how to report the results of this analysis:

[^correlation-2]: People occasionally report the value of the correlation coefficient, the *t*-statistic, the degrees of freedom, and the *p*-value. We won't do this.

> There is a negative correlation between bracken and heather among the study plots (r=-0.76, n=22, p \< 0.001).

Notice that we stated the direction of the association. After all, that's what matters to the land manager.

## Next steps

When we summarised the result, we did not say that bracken is having a negative *effect* on the heather, or *vice versa*. It might well be true that bracken has a negative effect on heather. However, our correlation analysis only characterises the association between bracken and heather[^correlation-3]. If we want to make statements about one species is related to the other, we need use a different kind of analysis. That's the focus of our next topic: simple linear regression.

[^correlation-3]: There is a second subtler limitation to this analysis: it was conducted using data from an observational study. If we want to say something about how bracken *affects* heather then we have to conduct an experimental manipulation (e.g. removing bracken from some plots).
